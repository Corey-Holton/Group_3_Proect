{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chris\\anaconda3\\envs\\audio_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Coremltools is not installed. If you plan to use a CoreML Saved Model, reinstall basic-pitch with `pip install 'basic-pitch[coreml]'`\n",
      "WARNING:root:tflite-runtime is not installed. If you plan to use a TFLite Model, reinstall basic-pitch with `pip install 'basic-pitch tflite-runtime'` or `pip install 'basic-pitch[tf]'\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "from basic_pitch.inference import predict_and_save, Model\n",
    "from basic_pitch import ICASSP_2022_MODEL_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `predict_and_save()` Parameters\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`audio_path_list: Sequence[Union[pathlib.Path, str]]`:  \n",
    "- **Description**: A list of file paths for the audio files to run inference on.\n",
    "- **Usage**: Provide a list of paths to the audio files you want to process. Each path can be a string or a `pathlib.Path` object.\n",
    "- **Example**: `[\"audio1.wav\", \"audio2.wav\"]` or `[pathlib.Path(\"audio1.wav\"), pathlib.Path(\"audio2.wav\")]`\n",
    "\n",
    "`output_directory: Union[pathlib.Path, str]`:\n",
    "- **Description**: The directory where the output files (MIDI, model outputs, etc.) will be saved.\n",
    "- **Usage**: Provide the path to the directory where you want to save the results. It can be a string or a \n",
    "`pathlib.Path` object.\n",
    "- **Example**: `\"output/\"` or `pathlib.Path(\"output/\")`\n",
    "\n",
    "`save_midi: bool`\n",
    "- **Description**: Whether to save the MIDI file generated from the predictions.\n",
    "- **Usage**: Set to `True` to save the MIDI file, `False` otherwise.\n",
    "\n",
    "`sonify_midi: bool`\n",
    "- **Description**: Whether to render audio from the MIDI and save it to a file.\n",
    "- **Usage**: Set to `True` to convert the MIDI to an audio file and save it, `False` otherwise.\n",
    "\n",
    "`save_model_outputs: bool`\n",
    "- **Description**: Whether to save the raw model outputs (contours, onsets, and notes) to a `.npz` file.\n",
    "- **Usage**: Set to `True` to save the model outputs, `False` otherwise.\n",
    "\n",
    "`save_notes: bool`\n",
    "- **Description**: Whether to save the note events to a file.\n",
    "- **Usage**: Set to `True` to save the note events, `False` otherwise.\n",
    "\n",
    "`model_or_model_path: Union[Model, str, pathlib.Path]`\n",
    "- **Description**: A loaded model or the path to a serialized model to load.\n",
    "- **Usage**: Provide either a model object or the path to the model file.\n",
    "- **Example**: `model_or_model_path=\"model.pth\"` or `model_or_model_path=model_instance`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate MIDI\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<basic_pitch.inference.Model at 0x25a1bd480d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_pitch_model = Model(ICASSP_2022_MODEL_PATH)\n",
    "\n",
    "basic_pitch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting MIDI for output_audio\\htdemucs_ft\\input_4\\vocals.mp3...\n",
      "\n",
      "\n",
      "  Creating midi...\n",
      "  ðŸ’… Saved to output_midi\\vocals_basic_pitch.mid\n"
     ]
    }
   ],
   "source": [
    "song = Path(\"./output_audio/htdemucs_ft/input_4/vocals.mp3\")\n",
    "output_path = Path(\"./output_midi\")\n",
    "\n",
    "predict_and_save(\n",
    "    audio_path_list=[song],\n",
    "    output_directory=output_path,\n",
    "    save_midi=True,\n",
    "    sonify_midi=False,\n",
    "    save_model_outputs=False,\n",
    "    save_notes=False,\n",
    "    model_or_model_path=basic_pitch_model,\n",
    "    onset_threshold=0.5,\n",
    "    frame_threshold=0.3,\n",
    "    minimum_note_length=127.70,\n",
    "    minimum_frequency=None,\n",
    "    maximum_frequency=None,\n",
    "    multiple_pitch_bends=False,\n",
    "    melodia_trick=True,\n",
    "    debug_file=None,\n",
    "    sonification_samplerate=44100,\n",
    "    midi_tempo=120,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `predict_and_save()` Optional Parameters\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`onset_threshold: float = 0.5`:\n",
    "- Description: The minimum energy required for an onset to be considered present.\n",
    "- Usage: Adjust this value to control the sensitivity of onset detection.\n",
    "- Example: \n",
    "\n",
    "`frame_threshold: float = 0.3`:\n",
    "- Description: The minimum energy required for a frame to be considered present.\n",
    "- Usage: Adjust this value to control the sensitivity of frame detection.\n",
    "\n",
    "`minimum_note_length: float = 127.70`:\n",
    "- Description: The minimum allowed note length in milliseconds.\n",
    "- Usage: Set the minimum duration for detected notes.\n",
    "\n",
    "`minimum_frequency: Optional[float] = None`:\n",
    "- Description: The minimum allowed output frequency in Hz. If `None`, all frequencies are used.\n",
    "- Usage: Set the minimum frequency for detected notes.\n",
    "- Example: \n",
    "\n",
    "`maximum_frequency: Optional[float] = None`:\n",
    "- Description: The maximum allowed output frequency in Hz. If `None`, all frequencies are used.\n",
    "- Usage: Set the maximum frequency for detected notes.\n",
    "\n",
    "`multiple_pitch_bends: bool = False`:\n",
    "- Description: Whether to allow overlapping notes in the MIDI file to have pitch bends.\n",
    "- Usage: Set to `True` to enable multiple pitch bends, `False` otherwise.\n",
    "\n",
    "`melodia_trick: bool = True`:\n",
    "- Description: Whether to use the melodia post-processing step.\n",
    "- Usage: Set to `True` to apply melodia post-processing, `False` otherwise.\n",
    "\n",
    "`debug_file: Optional[pathlib.Path] = None`:\n",
    "- Description: An optional path to output debug data to. Useful for testing and verification.\n",
    "- Usage: Provide a path to save debug information.\n",
    "- Example: `debug_file=pathlib.Path(\"debug.txt\")`\n",
    "\n",
    "`sonification_samplerate: int = 44100`:\n",
    "- Description: The sample rate for rendering audio from MIDI.\n",
    "- Usage: Set the sample rate for the audio file generated from the MIDI.\n",
    "\n",
    "`midi_tempo: float = 120`:\n",
    "- Description: The tempo for the generated MIDI file.\n",
    "- Usage: Set the tempo in beats per minute (BPM)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
